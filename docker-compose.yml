
services:
  db:
    image: postgres:15-alpine
    container_name: cirs-db
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-cirs}
      POSTGRES_USER: ${POSTGRES_USER:-cirs}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-cirs}
    volumes:
      - ./data/db:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-cirs}"]
      interval: 10s
      timeout: 5s
      retries: 5

  qdrant:
    image: qdrant/qdrant:latest
    container_name: cirs-qdrant
    ports:
      - "6333:6333"
    volumes:
      - ./data/qdrant:/qdrant/storage
  asr_gpu:
    build: ./backend/transcription/asr_gpu
    container_name: cirs-asr-gpu
    environment:
      - ASR_MODEL=${ASR_MODEL:-large-v3}
      - ASR_COMPUTE_TYPE=${ASR_COMPUTE_TYPE:-float16}
      - ASR_DIARIZATION=${ASR_DIARIZATION:-false}
      - INPUT_PATH=/data/ingestion
      - OUTPUT_PATH=/data/transcripts
      - DB_URL=${DB_URL}
      - PYANNOTE_AUTH_TOKEN=${PYANNOTE_AUTH_TOKEN}
      - RUN_TAG=${RUN_TAG}
    volumes:
      - ./data:/data
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    ports:
      - "8001:8001"
    command: ["python3", "main.py", "--input", "/data/ingestion"]

  validation_gpu:
    build: ./backend/processing/validation_gpu
    container_name: cirs-validation-gpu
    environment:
      - INPUT_PATH=/data/transcripts
      - OUTPUT_PATH=/data/validated
      - DB_URL=${DB_URL}
      - VALIDATION_MODEL=${VALIDATION_MODEL:-biogpt}
      - VALIDATION_THRESHOLD=${VALIDATION_THRESHOLD:-0.85}
      - UMLS_PATH=${UMLS_PATH:-/models/umls}
      - USE_GPU=${USE_GPU:-true}
      - RUN_TAG=${RUN_TAG}
    volumes:
      - ./data:/data
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    command: ["python3", "main.py", "--input", "/data/transcripts"]

  chunking_embeddings_gpu:
    build: ./backend/processing/chunking_embeddings_gpu
    container_name: cirs-chunking-embeddings-gpu
    environment:
      - INPUT_PATH=/data/validated
      - OUTPUT_PATH=/data/chunks
      - DB_URL=${DB_URL}
      - QDRANT_URL=${QDRANT_URL:-http://qdrant:6333}
      - QDRANT_COLLECTION=${QDRANT_COLLECTION:-cirs_chunks_v1}
      - EMBED_MODEL=${EMBED_MODEL:-BAAI/bge-large-en-v1.5}
      - EMBED_BATCH_SIZE=${EMBED_BATCH_SIZE:-16}
      - CHUNK_SIZE_TOKENS=${CHUNK_SIZE_TOKENS:-350}
      - CHUNK_OVERLAP_TOKENS=${CHUNK_OVERLAP_TOKENS:-50}
      - RUN_TAG=${RUN_TAG}
    volumes:
      - ./data:/data
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    command: ["python3", "main.py", "--input", "/data/validated"]

  hybrid_retriever:
    build: ./backend/retrieval/hybrid_retriever
    container_name: cirs-hybrid-retriever
    environment:
      - QDRANT_URL=${QDRANT_URL:-http://qdrant:6333}
      - QDRANT_COLLECTION=${QDRANT_COLLECTION:-cirs_chunks_v1}
      - BM25_INDEX_PATH=${BM25_INDEX_PATH:-/data/index/bm25}
      - EMBED_MODEL=${EMBED_MODEL:-BAAI/bge-large-en-v1.5}
      - VECTOR_WEIGHT=${VECTOR_WEIGHT:-0.6}
      - LEXICAL_WEIGHT=${LEXICAL_WEIGHT:-0.4}
      - TOP_K_VECTOR=${TOP_K_VECTOR:-20}
      - TOP_K_LEXICAL=${TOP_K_LEXICAL:-20}
    volumes:
      - ./data:/data
    depends_on:
      - qdrant
    ports:
      - "8002:8002"

  chat_orchestrator:
    build: ./backend/chat_orchestrator
    container_name: cirs-chat-orchestrator
    environment:
      - RETRIEVER_URL=${RETRIEVER_URL:-http://hybrid_retriever:8002}
      - LLM_MODE=${LLM_MODE:-local}
      - LLM_LOCAL_URL=${LLM_LOCAL_URL:-http://ollama:11434}
      - LLM_LOCAL_MODEL=${LLM_LOCAL_MODEL:-llama3.1:8b-instruct}
      - LLM_REMOTE_URL=${LLM_REMOTE_URL:-https://api.openai.com/v1/chat/completions}
      - LLM_REMOTE_MODEL=${LLM_REMOTE_MODEL:-gpt-4o-mini}
      - LLM_API_KEY=${LLM_API_KEY}
      - MAX_TOKENS=${MAX_TOKENS:-1500}
      - TEMPERATURE=${TEMPERATURE:-0.3}
      - TOP_K=${TOP_K:-6}
    depends_on:
      - hybrid_retriever
    volumes:
      - ./data:/data
    ports:
      - "8003:8003"

  ui_web:
    build: ./frontend/chat_ui
    container_name: cirs-ui-web
    environment:
      - VITE_API_URL=${VITE_API_URL:-http://chat_orchestrator:8003}
      - VITE_DEFAULT_MODE=${VITE_DEFAULT_MODE:-hybrid}
      - VITE_MAX_TOKENS=${VITE_MAX_TOKENS:-1500}
      - VITE_TEMPERATURE=${VITE_TEMPERATURE:-0.7}
    depends_on:
      - chat_orchestrator
    ports:
      - "5173:80"

  monitoring:
    build: ./backend/monitoring
    container_name: cirs-monitoring
    environment:
      - DB_URL=${DB_URL}
      - QDRANT_URL=${QDRANT_URL:-http://qdrant:6333}
    depends_on:
      - qdrant
    ports:
      - "8010:8010"

  monitoring_ui:
    build: ./frontend/monitoring_ui
    container_name: cirs-monitoring-ui
    depends_on:
      - monitoring
    ports:
      - "5174:80"

  evaluation:
    build: ./backend/evaluation
    container_name: cirs-evaluation
    environment:
      - DB_URL=${DB_URL}
      - QDRANT_URL=${QDRANT_URL:-http://qdrant:6333}
      - EMBED_MODEL=${EMBED_MODEL:-BAAI/bge-large-en-v1.5}
      - USE_GPU=${USE_GPU:-true}
    volumes:
      - ./data:/data
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    ports:
      - "8011:8011"

  alignment_qa:
    build: ./backend/alignment_qa
    container_name: cirs-alignment-qa
    environment:
      - DB_URL=${DB_URL}
      - QDRANT_URL=${QDRANT_URL:-http://qdrant:6333}
      - EMBED_MODEL=${EMBED_MODEL:-BAAI/bge-large-en-v1.5}
      - USE_GPU=${USE_GPU:-true}
    volumes:
      - ./data:/data
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    ports:
      - "8012:8012"

  reinforcement:
    build: ./backend/reinforcement
    container_name: cirs-reinforcement
    environment:
      - DB_URL=${DB_URL}
      - CONFIG_PATH=${CONFIG_PATH:-./backend/chat_orchestrator/config.yaml}
    volumes:
      - ./backend:/backend
      - ./data:/data
    ports:
      - "8013:8013"

  feedback:
    build: ./backend/feedback
    container_name: cirs-feedback
    environment:
      - DB_URL=${DB_URL}
    depends_on:
      - db
    volumes:
      - ./data:/data
    ports:
      - "8014:8014"

  finetune:
    build: ./backend/finetune
    container_name: cirs-finetune
    environment:
      - DB_URL=${DB_URL}
      - FINETUNE_ENABLED=${FINETUNE_ENABLED:-false}
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    volumes:
      - ./data:/data
      - ./backend:/backend
    ports:
      - "8015:8015"

  security_guardrails:
    build: ./backend/security_guardrails
    container_name: cirs-security-guardrails
    environment:
      - DB_URL=${DB_URL}
      - REDACTION_LOG_PATH=/data/security/redaction_log.json
    volumes:
      - ./data:/data
    ports:
      - "8016:8016"

  license_audit:
    build: ./backend/license_audit
    container_name: cirs-license-audit
    environment:
      - DB_URL=${DB_URL}
    volumes:
      - ./data:/data
    ports:
      - "8017:8017"
